{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Spark sanity-check notebook\n", "\n", "This notebook creates a SparkSession against your cluster and runs a tiny job. It also enables event logging so the Spark History Server can display the run.\n", "\n", "**Tips**:\n", "- When running *inside the Docker JupyterLab* container, set `SPARK_MASTER_URL` to `spark://spark-master:7077` (already set by Terraform). The event logs will go to `/opt/bitnami/spark/tmp/spark-events`.\n", "- When running *on the host*, set `SPARK_MASTER_URL` to `spark://localhost:7077`. The notebook will fallback to a writable local folder (e.g. `/tmp/spark-events`) if the Bitnami path isn't writable.\n", "- Make sure a Java runtime (e.g. OpenJDK 17) is installed when running on the host.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, pathlib, sys\n", "from pyspark.sql import SparkSession\n", "\n", "def pick_event_log_dir():\n", "    candidates = [\n", "        \"file:/opt/bitnami/spark/tmp/spark-events\",  # Docker Bitnami image path\n", "        \"file:/event-logs\",                           # extra mount (optional)\n", "        \"file:/tmp/spark-events\",                    # host fallback\n", "    ]\n", "    for uri in candidates:\n", "        path = uri.replace(\"file:\", \"\")\n", "        try:\n", "            p = pathlib.Path(path)\n", "            p.mkdir(parents=True, exist_ok=True)\n", "            with open(p/\"._write_test\", \"w\") as f:\n", "                f.write(\"ok\")\n", "            return uri\n", "        except Exception:\n", "            continue\n", "    return \"file:/tmp/spark-events\"\n", "\n", "master = os.getenv(\"SPARK_MASTER_URL\", \"spark://localhost:7077\")\n", "event_log_dir = pick_event_log_dir()\n", "\n", "spark = (SparkSession.builder\n", "         .master(master)\n", "         .appName(\"hist-check\")\n", "         .config(\"spark.eventLog.enabled\", \"true\")\n", "         .config(\"spark.eventLog.dir\", event_log_dir)\n", "         .getOrCreate())\n", "\n", "print(\"Spark version:\", spark.version)\n", "print(\"Master:\", master)\n", "print(\"eventLog.dir:\", event_log_dir)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run a tiny job\n", "df = spark.range(100000)\n", "df.selectExpr(\"sum(id) AS total\").show()\n", "print(\"Done.\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Clean shutdown (optional)\n", "spark.stop()\n", "print(\"Spark stopped.\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}